{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降低损失 (Reducing Loss)\n",
    "    为了训练模型，我们需要一种可降低模型损失的好方法。迭代方法是一种广泛用于降低损失的方法，而且使用起来简单有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [视频讲座](https://developers.google.cn/machine-learning/crash-course/reducing-loss/video-lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [迭代方法](https://developers.google.cn/machine-learning/crash-course/reducing-loss/an-iterative-approach)\n",
    "\n",
    "### 机器学习算法用于训练模型的迭代试错过程：\n",
    "![](images/03_training_model_process.png)\n",
    "\n",
    "* 对于**线性回归**问题，事实证明初始值并不重要。我们可以随机选择值或者设置为0。\n",
    "* 通常，您可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已**收敛**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [梯度下降法](https://developers.google.cn/machine-learning/crash-course/reducing-loss/gradient-descent)\n",
    "\n",
    "![](images/03_loss_weight.png)\n",
    "\n",
    "* 凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处。\n",
    "\n",
    "* 梯度下降法\n",
    "    1. 梯度下降法的第一个阶段是为 W1 选择一个起始值（起点）。起点并不重要；因此很多算法就直接将 W1 设为 0 或随机选择一个值。\n",
    "    2. 然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，梯度是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度就等于导数。\n",
    "\n",
    "* 梯度是一个矢量，因此具有以下两个特征：\n",
    "    * 方向\n",
    "    * 大小\n",
    "\n",
    "* 梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。\n",
    "* 为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加。\n",
    "* 梯度下降法会重复此过程，逐渐接近最低点。\n",
    "\n",
    "![](images/03_loss_weight5.png)\n",
    "\n",
    "### 偏导数和梯度\n",
    "![](images/03_partial_derivatives_and_gradient1.png)\n",
    "![](images/03_partial_derivatives_and_gradient2.png)\n",
    "![](images/03_partial_derivatives_and_gradient3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [学习速率](https://developers.google.cn/machine-learning/crash-course/reducing-loss/learning-rate)\n",
    "\n",
    "* **梯度下降法算法**用**梯度 乘**以一个称为**学习速率**（有时也称为步长）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。\n",
    "* **超参数**是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果您选择的学习**速率过小**，就会花费太长的学习时间。相反，如果您指定的学习**速率过大**，下一个点将永远在 U 形曲线的底部随意弹跳。\n",
    "* 每个回归问题都存在一个金发姑娘学习速率。“金发姑娘”值与损失函数的平坦程度相关。如果您知道损失函数的梯度较小，则可以放心地试着采用更大的学习速率，以补偿较小的梯度并获得更大的步长。\n",
    "\n",
    "![](images/03_ideal_learning_rate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [随机梯度下降法](https://developers.google.cn/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent)\n",
    "\n",
    "* 包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。\n",
    "* 如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 **随机梯度下降法 (SGD)** 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。\n",
    "* **小批量随机梯度下降法（小批量 SGD）**是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 **10-1000** 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。\n",
    "* 梯度下降法也适用于包含多个特征的特征集。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
